{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了实现集装箱锂离子电池储能系统领域火灾爆炸事故报告的RAG构建，我们使用以下代码实现：\n",
    "\n",
    "注：我们选取2019年美国亚利桑那McMicken电池储能系统火灾事故、2020年英国利物浦Carnegie Road电池储能系统火灾事故、2021年澳大利亚维多利亚大电池火灾事故、2023年瑞典哥德堡集装箱电池储能系统火灾事故为例进行研究。相关文件在文件夹中。"
   ],
   "id": "1069c6471f9ec41b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "以下代码在Colab中运行。",
   "id": "500c5971c8ed4ee4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "（1）安装相关包",
   "id": "6fe60b77d64b6301"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install langchain_openai\n",
    "!pip install langchain_chroma\n",
    "!pip install langchain_community\n",
    "!pip install pypdf\n",
    "!pip install chromadb\n",
    "!pip install langgraph"
   ],
   "id": "efa88793dd74e4d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "（2）设置环境变量",
   "id": "11ccef69186d8369"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '你的OPENAI_API_KEY'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '你的LANGCHAIN_API_KEY'"
   ],
   "id": "67bd8fbea2e5b984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "（3）将文件添加至Chroma（此处将文件分段处理，也可以不分段）",
   "id": "66073b18fada9778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"run_api\")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "reports_client_path = r\"指定的db路径\"\n",
    "reports_client = chromadb.PersistentClient(path=reports_client_path,settings=Settings(allow_reset=True))\n",
    "reports_client.reset()\n",
    "\n",
    "collection_name = \"accident_reports_collection\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "accident_reports_collection = reports_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=reports_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "print(\"run_vectorstore\")\n",
    "\n",
    "\"\"\"加载pdf文件\"\"\"\n",
    "def load_pdf(file_path):\n",
    "    # 创建 PyPDFLoader 对象\n",
    "    loader = PyPDFLoader(file_path=file_path)\n",
    "    pages = []\n",
    "    # 按页码依次处理\n",
    "    for page in loader.lazy_load():\n",
    "        pages.append(page)\n",
    "    print(\"run_loader\")\n",
    "    return pages\n",
    "\n",
    "\"\"\"分割文本为更小的块\"\"\"\n",
    "def split_pdf(data):\n",
    "    pdf_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    print(\"run_split_pdf\")\n",
    "    return pdf_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "def data_process(file_path,file_information):\n",
    "    \"\"\"主函数，处理用户输入\"\"\"\n",
    "    # 加载并处理数据\n",
    "    data = load_pdf(file_path)\n",
    "    print(len(data))\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(data), batch_size):\n",
    "      start_index = 0\n",
    "      end_index = min(start_index + batch_size, len(data))\n",
    "      batch = data[start_index:end_index]\n",
    "      splits = split_pdf(batch)\n",
    "      id_batches=[]\n",
    "      for _ in range(len(splits)):\n",
    "          id_batch = f\"{file_information}_{start_index}_{end_index}_{_}\"\n",
    "          id_batches.append(id_batch)\n",
    "      vectorstore.add_documents(documents=splits, ids=id_batches,collection_name=collection_name)\n",
    "      print(\"run_add\")\n",
    "    print(\"run_data_process\")\n",
    "\n",
    "def add_reports():\n",
    "    file_path_1 = r\"/content/drive/MyDrive/Colab Notebooks/Four_Firefighters_Injured_In_Lithium_Ion_Battery_ESS_Explosion_Arizona_0.pdf\"\n",
    "    file_path_2 = r\"/content/drive/MyDrive/Colab Notebooks/2020-09-15 UK, Liverpool - Investigative Report.pdf\"\n",
    "    file_path_3 = r\"/content/drive/MyDrive/Colab Notebooks/2021-07-30 Australia, Victoria, Moorabool - Investigation Report.pdf\"\n",
    "    file_path_4 = r\"/content/drive/MyDrive/Colab Notebooks/2023-04-26 Sweden, Gothenburg - Investigation Report (English Translation).pdf\"\n",
    "\n",
    "    data_process(file_path_1,\"2019_US\")\n",
    "    print(\"run_1\")\n",
    "    data_process(file_path_2,\"2020_UK\")\n",
    "    print(\"run_2\")\n",
    "    data_process(file_path_3,\"2021_Australia\")\n",
    "    print(\"run_3\")\n",
    "    data_process(file_path_4,\"2023_Sweden\")\n",
    "    print(\"run_4\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_reports()"
   ],
   "id": "52ec155832d9bb40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "（4）检查是否创建集合\n",
    "\n",
    "输出：[Collection(name=accident_reports_collection)]"
   ],
   "id": "6c3452529a761c30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "reports_client_path = r\"指定的db路径\"\n",
    "reports_client = chromadb.PersistentClient(path=reports_client_path,settings=Settings(allow_reset=True))\n",
    "collections = reports_client.list_collections()\n",
    "print(collections)"
   ],
   "id": "80639fd2f2ff1c04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "（5）使用hub提示模版\n",
    "\n",
    "该提示模版要求大模型使用三句话回答。"
   ],
   "id": "525a19f6fb618434"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"使用hub中的提示模版\"\"\"\n",
    "from langchain import hub\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langsmith import Client\n",
    "api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "print(os.getenv('LANGCHAIN_API_KEY'))\n",
    "client = Client(api_key=api_key)\n",
    "print(\"LangSmith client initialized successfully!\")\n",
    "print(\"run_api\")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "reports_client_path = r\"指定的db路径\"\n",
    "reports_client = chromadb.PersistentClient(path=reports_client_path,settings=Settings(allow_reset=True))\n",
    "\n",
    "collection_name = \"accident_reports_collection\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "accident_reports_collection = reports_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=reports_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "# print(vectorstore)\n",
    "print(\"run_vectorstore\")\n",
    "\n",
    "def run():\n",
    "    # 初始化问答链\n",
    "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.9, max_tokens = 4000)\n",
    "  # qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "  # 用户提问循环\n",
    "  while True:\n",
    "      query = input(\"请输入问题，例如：维多利亚大电池火灾事故发生的原因是什么？\\n\")\n",
    "      if query.lower() == \"end\":\n",
    "          print(\"程序结束！\")\n",
    "          break\n",
    "\n",
    "      print(f\"问题：{query}\")\n",
    "      # answer = qa_chain.invoke({\"query\": query})\n",
    "      prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "      retrieved_docs = vectorstore.similarity_search(query=query, k=15)\n",
    "      print(\"相似性文档如下：\", retrieved_docs)\n",
    "      docs_content = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "      messages = prompt.invoke({\"question\": query, \"context\": docs_content})\n",
    "      # invoke 的作用是：基于提供的 question 和 context，生成一个适合的提示。它可能会将 query 和 docs_content 结合成一个自然语言提示\n",
    "      response = llm.invoke(messages)\n",
    "      print(f\"回答：{response.content}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "ac6517d986f6bbcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "（6）使用hub中的提示模版，并定义状态类",
   "id": "e54bfc8923f1c7f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"使用hub中的提示模版，并定义状态类\"\"\"\n",
    "from langchain import hub\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langsmith import Client\n",
    "api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "print(os.getenv('LANGCHAIN_API_KEY'))\n",
    "client = Client(api_key=api_key)\n",
    "print(\"LangSmith client initialized successfully!\")\n",
    "print(\"run_api\")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "reports_client_path = r\"指定的db路径\"\n",
    "reports_client = chromadb.PersistentClient(path=reports_client_path,settings=Settings(allow_reset=True))\n",
    "\n",
    "collection_name = \"accident_reports_collection\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "accident_reports_collection = reports_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=reports_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "# print(vectorstore)\n",
    "print(\"run_vectorstore\")\n",
    "\n",
    "def run():\n",
    "    # 初始化问答链\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    from langchain_core.documents import Document\n",
    "    from typing_extensions import List, TypedDict\n",
    "    from langgraph.graph import START, StateGraph\n",
    "\n",
    "    class State(TypedDict):\n",
    "        question: str\n",
    "        context: List[Document]\n",
    "        answer: str\n",
    "\n",
    "    def retrieve(state: State):\n",
    "        retrieved_docs = vectorstore.similarity_search(state[\"question\"], k=15, filter = {\"source\": source} )\n",
    "        print(\"相似性文档如下：\", retrieved_docs)\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "    def generate(state: State):\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "        messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "        response = llm.invoke(messages)\n",
    "        return {\"answer\": response.content}\n",
    "  # 用户提问循环\n",
    "    while True:\n",
    "        query = input(\"请输入问题，例如：维多利亚大电池火灾事故发生的原因是什么？\\n\")\n",
    "        if query.lower() == \"end\":\n",
    "            print(\"程序结束！\")\n",
    "            break\n",
    "        print(f\"问题：{query}\")\n",
    "        source = input(\"请输入数据来源，例如：/content/drive/MyDrive/Colab Notebooks/2019-04-19 US, AZ, Surprise - Investigation.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2020-09-15 UK, Liverpool - Investigative Report.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2021-07-30 Australia, Victoria, Moorabool - Investigation Report.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2023-04-26 Sweden, Gothenburg - Investigation Report (English Translation).pdf\\n\")\n",
    "        print(f\"问题：{source}\")\n",
    "\n",
    "        graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        graph_builder.add_edge(START, \"retrieve\")\n",
    "        graph = graph_builder.compile()\n",
    "\n",
    "        response = graph.invoke({\"question\": query})\n",
    "        print(response[\"answer\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "c804519b8d611904"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "（7）使用自定义提示模版，性能较好",
   "id": "1ae78b12b4828979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"使用自定义提示模版，性能较好\"\"\"\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langsmith import Client\n",
    "api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "client = Client(api_key=api_key)\n",
    "print(\"run_api\")\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "reports_client_path = r\"指定的db路径\"\n",
    "reports_client = chromadb.PersistentClient(path=reports_client_path,settings=Settings(allow_reset=True))\n",
    "\n",
    "collection_name = \"accident_reports_collection\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "accident_reports_collection = reports_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=reports_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "# print(vectorstore)\n",
    "print(\"run_vectorstore\")\n",
    "\n",
    "def run():\n",
    "    # 初始化问答链\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "    # prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    # You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use 1000 words maximum and keep the answer concise.\n",
    "    template = \"\"\"你是一个问答任务的助手。请使用以下信息回答问题。如果不知道，则直接总结question和context的内容。请使用三句话回答并确保回答的准确性。\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    class State(TypedDict):\n",
    "        question: str\n",
    "        context: List[Document]\n",
    "        answer: str\n",
    "\n",
    "    def retrieve(state: State):\n",
    "        retrieved_docs = vectorstore.similarity_search(state[\"question\"], k=15, filter = {\"source\": source} )\n",
    "        print(\"相似性文档如下：\", retrieved_docs)\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "    def generate(state: State):\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "        messages = custom_rag_prompt.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": docs_content})\n",
    "        print(messages)\n",
    "        response = llm.invoke(messages)\n",
    "        print(response)\n",
    "        return {\"answer\": response.content}\n",
    "  # 用户提问循环\n",
    "    while True:\n",
    "        query = input(\"请输入问题，例如：维多利亚大电池火灾事故发生的原因是什么？\\n\")\n",
    "        if query.lower() == \"end\":\n",
    "            print(\"程序结束！\")\n",
    "            break\n",
    "        print(f\"问题：{query}\")\n",
    "        source = input(\"请输入数据来源，例如：/content/drive/MyDrive/Colab Notebooks/2019-04-19 US, AZ, Surprise - Investigation.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2020-09-15 UK, Liverpool - Investigative Report.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2021-07-30 Australia, Victoria, Moorabool - Investigation Report.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2023-04-26 Sweden, Gothenburg - Investigation Report (English Translation).pdf\\n\")\n",
    "        print(f\"数据来源：{source}\")\n",
    "\n",
    "        graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        graph_builder.add_edge(START, \"retrieve\")\n",
    "        graph = graph_builder.compile()\n",
    "\n",
    "        response = graph.invoke({\"question\": query})\n",
    "        print(response[\"answer\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "7a8c10d04098e03d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "（8）使用OpenAI的API请求",
   "id": "549cd8d4c52cc6a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"使用OpenAI的API请求\"\"\"\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "from langsmith import Client\n",
    "langsmith_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "# print(\"run_api\")\n",
    "\n",
    "from openai import OpenAI\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "reports_client_path = r\"指定的db路径\"\n",
    "reports_client = chromadb.PersistentClient(path=reports_client_path,settings=Settings(allow_reset=True))\n",
    "\n",
    "collection_name = \"accident_reports_collection\"\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "accident_reports_collection = reports_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=reports_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "# print(\"run_vectorstore\")\n",
    "\n",
    "def run():\n",
    "    template = \"\"\"你是一个问答任务的助手。请使用以下信息用中文回答问题。如果无法使用以下信息回答，则输出：无法根据信息回答。并直接根据Question内容回答。请使500字左右回答并确保回答的准确性。\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    class State(TypedDict):\n",
    "        question: str\n",
    "        context: List[Document]\n",
    "        answer: str\n",
    "\n",
    "    def retrieve(state: State):\n",
    "        retrieved_docs = vectorstore.similarity_search(state[\"question\"], k=15, filter = {\"source\": source} )\n",
    "        # print(\"相似性文档如下：\", retrieved_docs)\n",
    "        return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "    def generate(state: State):\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "        messages = custom_rag_prompt.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": docs_content})\n",
    "        # print(messages)\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": messages.text\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        # response = llm.invoke(messages)\n",
    "        # print(completion.choices[0].message.content)\n",
    "        # print(response)\n",
    "        return {\"answer\": completion.choices[0].message.content}\n",
    "  # 用户提问循环\n",
    "    while True:\n",
    "        query = input(\"请输入问题，例如：维多利亚大电池火灾事故发生的原因是什么？\\n\")\n",
    "        if query.lower() == \"end\":\n",
    "            print(\"程序结束！\")\n",
    "            break\n",
    "        print(f\"问题：{query}\")\n",
    "        source_num = int(input(\"请输入数据来源：\\n\"))\n",
    "        # 例如：/content/drive/MyDrive/Colab Notebooks/2019-04-19 US, AZ, Surprise - Investigation.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2020-09-15 UK, Liverpool - Investigative Report.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2021-07-30 Australia, Victoria, Moorabool - Investigation Report.pdf\\n 或 /content/drive/MyDrive/Colab Notebooks/2023-04-26 Sweden, Gothenburg - Investigation Report (English Translation).pdf\\n\")\n",
    "        if source_num == 1:\n",
    "          source = \"/content/drive/MyDrive/Colab Notebooks/Four_Firefighters_Injured_In_Lithium_Ion_Battery_ESS_Explosion_Arizona_0.pdf\"\n",
    "        elif source_num == 2:\n",
    "          source = \"/content/drive/MyDrive/Colab Notebooks/2020-09-15 UK, Liverpool - Investigative Report.pdf\"\n",
    "        elif source_num == 3:\n",
    "          source = \"/content/drive/MyDrive/Colab Notebooks/2021-07-30 Australia, Victoria, Moorabool - Investigation Report.pdf\"\n",
    "        elif source_num == 4:\n",
    "          source = \"/content/drive/MyDrive/Colab Notebooks/2023-04-26 Sweden, Gothenburg - Investigation Report (English Translation).pdf\"\n",
    "        print(f\"数据来源：{source}\")\n",
    "\n",
    "        graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "        graph_builder.add_edge(START, \"retrieve\")\n",
    "        graph = graph_builder.compile()\n",
    "\n",
    "        response = graph.invoke({\"question\": query})\n",
    "        print(response[\"answer\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "9842ccfb54a30714"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
