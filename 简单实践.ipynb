{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先，尝试OpenAI的API请求：",
   "id": "f590ad9930593524"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T09:20:48.023801Z",
     "start_time": "2025-01-17T09:20:43.325773Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "api_key = \"你的API密钥\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"美国总统是谁？\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021年，美国总统是乔·拜登（Joe Biden）。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "回答：2021年，美国总统是乔·拜登（Joe Biden）。则说明API请求成功。\n",
    "接下来，尝试使用qa.txt文件实现一个简单的RAG。这里，我们使用Chroma数据库进行向量存储。"
   ],
   "id": "798c2a4cc840eecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"加载文本数据\"\"\"\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "def split_text(data):\n",
    "    \"\"\"分割文本为更小的块\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "def create_vectorstore(splits):\n",
    "    persist_directory = r\"D:\\good_good_study\\111cs_study\\Python\\LLM-langchain\\mydb.db\"  # 指定存储文件的路径\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, embedding=OpenAIEmbeddings(), persist_directory=persist_directory\n",
    "    )\n",
    "    \"\"\"创建 Chroma 向量存储\"\"\"\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def answer_question(question, qa_chain):\n",
    "    \"\"\"获取问题的答案\"\"\"\n",
    "    try:\n",
    "        result = qa_chain({\"query\": question})\n",
    "        return result['result']\n",
    "    except Exception as e:\n",
    "        return f\"发生错误：{e}\"\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"主函数，处理用户输入\"\"\"\n",
    "    # 加载并处理数据\n",
    "    data = load_data('qa.txt')\n",
    "    all_splits = split_text(data)\n",
    "    vectorstore = create_vectorstore(all_splits)\n",
    "\n",
    "    # 初始化问答链\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "\n",
    "    # 用户提问循环\n",
    "    while True:\n",
    "        # print(\"run\")\n",
    "        query = input(\"请输入问题，例如：弦丝画制作的活动时长是多少？\\n\")\n",
    "        if query.lower() == \"end\":\n",
    "            print(\"程序结束！\")\n",
    "            break\n",
    "\n",
    "        print(f\"问题：{query}\")\n",
    "        answer = qa_chain.invoke({\"query\": query})\n",
    "        print(f\"回答：{answer['result']}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "8a6d95c9acbd0bb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "下面逐段解释以上代码：",
   "id": "1300a28efe671138"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")",
   "id": "d8e8eada9dbcb847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这段代码获取OpenAI的API密钥，以便后续代码调用OpenAI的大模型。需要注意正确设置环境变量OPENAI_API_KEY，建议在Jupyter Notebook或Colab notebook中运行，方便设置环境变量。\n",
    "在Jupyter Notebook或Colab notebook中，可以通过以下代码段设置环境变量。"
   ],
   "id": "b88d5b0630c44e5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '你的OpenAI_API_KEY'"
   ],
   "id": "8e731e2ff00d80bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ],
   "id": "75947c6dc4b1859e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "151efeacbc25b35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
