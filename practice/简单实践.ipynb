{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "首先，尝试OpenAI的API请求：",
   "id": "904a7e10f988c4a9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T09:20:48.023801Z",
     "start_time": "2025-01-17T09:20:43.325773Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "api_key = \"你的API密钥\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"美国总统是谁？\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021年，美国总统是乔·拜登（Joe Biden）。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "回答：2021年，美国总统是乔·拜登（Joe Biden）。则说明API请求成功。\n",
    "接下来，尝试基于LangChain，使用qa.txt文件实现一个简单的RAG。这里，我们使用Chroma数据库进行向量存储。\n",
    "此处的实践参考来源于https://www.cnblogs.com/hailexuexi/p/18087721"
   ],
   "id": "2d2f4a47224aae88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\"\"\"忽略warnings\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"加载文本数据\"\"\"\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "def split_text(data):\n",
    "    \"\"\"分割文本为更小的块\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "def create_vectorstore(splits):\n",
    "    persist_directory = r\"D:\\good_good_study\\111cs_study\\Python\\LLM-langchain\\mydb.db\"  # 指定存储文件的路径\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, embedding=OpenAIEmbeddings(), persist_directory=persist_directory\n",
    "    )\n",
    "    \"\"\"创建 Chroma 向量存储\"\"\"\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def answer_question(question, qa_chain):\n",
    "    \"\"\"获取问题的答案\"\"\"\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result['result']\n",
    "\n",
    "def run():\n",
    "    \"\"\"主函数，处理用户输入\"\"\"\n",
    "    # 加载并处理数据\n",
    "    data = load_data('practice/qa.txt')\n",
    "    all_splits = split_text(data)\n",
    "    vectorstore = create_vectorstore(all_splits)\n",
    "\n",
    "    # 初始化问答链\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "\n",
    "    # 用户提问循环\n",
    "    while True:\n",
    "        # print(\"run\")\n",
    "        query = input(\"请输入问题，例如：弦丝画制作的活动时长是多少？\\n\")\n",
    "        if query.lower() == \"end\":\n",
    "            print(\"程序结束！\")\n",
    "            break\n",
    "\n",
    "        print(f\"问题：{query}\")\n",
    "        answer = qa_chain.invoke({\"query\": query})\n",
    "        print(f\"回答：{answer['result']}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "8a6d95c9acbd0bb2"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "下面逐段解释以上代码：",
   "id": "2537bffb8755f31b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "openai.api_key = os.getenv(\"OPENAI_API_KEY\")",
   "id": "d8e8eada9dbcb847"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "这段代码获取OpenAI的API密钥，以便后续代码调用OpenAI的大模型。需要注意正确设置环境变量OPENAI_API_KEY，建议在Jupyter Notebook或Colab notebook中运行，方便设置环境变量。\n",
    "在Jupyter Notebook或Colab notebook中，可以通过以下代码段设置环境变量："
   ],
   "id": "9f751b4c829a985c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '你的OpenAI_API_KEY'"
   ],
   "id": "8e731e2ff00d80bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ],
   "id": "75947c6dc4b1859e"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "忽略warnings，可注释掉。",
   "id": "3a6bc29708683dfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"加载文本数据\"\"\"\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    return loader.load()"
   ],
   "id": "ddf58c3c06bea38f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "加载文本的函数，这里使用的文件为qa.txt文件，所以使用TextLoader。为了避免乱码，可查看文件字符编码方式并设置（例如此处的encoding='utf-8'）。",
   "id": "922e4a19b01ed2d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_text(data):\n",
    "    \"\"\"分割文本为更小的块\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)"
   ],
   "id": "ca631060b4fd26ae"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "将加载的文件分割为文本块，chunk_size=50表示文本块大小为50个字符，chunk_overlap=0表示重叠的字符为0。则表示第1-50字符为第一个文本块，51-100字符为第二个文本块，以此类推。",
   "id": "ff197eac4d98a667"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_vectorstore(splits):\n",
    "    persist_directory = r\"你想指定的存储路径\"  # 指定存储文件的路径\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, embedding=OpenAIEmbeddings(), persist_directory=persist_directory\n",
    "    )\n",
    "    \"\"\"创建 Chroma 向量存储\"\"\"\n",
    "    return vectorstore"
   ],
   "id": "e77326e97629d6f8"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "创建向量数据库，此处使用Chroma数据库。此处指定了嵌入函数为OpenAIEmbeddings，并指定了向量库的路径。",
   "id": "5480d33855309dc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def answer_question(question, qa_chain):\n",
    "    \"\"\"获取问题的答案\"\"\"\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result['result']"
   ],
   "id": "cbc49d535ea939e1"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "获取用户输入的问题的答案。",
   "id": "a09412c49b8e2aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run():\n",
    "    \"\"\"主函数，处理用户输入\"\"\"\n",
    "    # 加载并处理数据\n",
    "    data = load_data('practice/qa.txt')\n",
    "    all_splits = split_text(data)\n",
    "    vectorstore = create_vectorstore(all_splits)\n",
    "\n",
    "    # 初始化问答链\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "\n",
    "    # 用户提问循环\n",
    "    while True:\n",
    "        # print(\"run\")\n",
    "        query = input(\"请输入问题，例如：弦丝画制作的活动时长是多少？\\n\")\n",
    "        if query.lower() == \"end\":\n",
    "            print(\"程序结束！\")\n",
    "            break\n",
    "\n",
    "        print(f\"问题：{query}\")\n",
    "        answer = qa_chain.invoke({\"query\": query})\n",
    "        print(f\"回答：{answer['result']}\")"
   ],
   "id": "f2b1dbc3af34640f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "主函数，使用的大模型为gpt-3.5-turbo。\n",
    "data为加载的qa文件，splits为分割后的文本块，向量库为储存有splits的向量库。\n",
    "运行，输入问题后，则可根据qa文件内容输出回答。"
   ],
   "id": "db929aa3d8932ad6"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "以上RAG程序实现后，则可类推到其他简单RAG应用。",
   "id": "31a3a9e41a2bf586"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
